{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c65779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# for better display of plots\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 200)\n",
    "#pd.set_option('display.max_rows', None) \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df = pd.read_csv(\"../data/final_cohort_main_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74257e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1083b5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "df_dupes = df[df.duplicated(subset=['SUBJECT_ID'], keep=False)].sort_values(by='SUBJECT_ID')\n",
    "print(df_dupes.shape)\n",
    "print(df_dupes.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9018e759",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4609ae1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to reduce the name of the columns -> we remove the sufix _val\n",
    "df = df.rename(\n",
    "    columns=lambda c: c[:-4] if c.endswith('_val') else c\n",
    ")\n",
    "\n",
    "if 'GENDER' in df.columns:\n",
    "    df['is_male'] = df['GENDER'].map({'M': 1, 'F': 0})\n",
    "    \n",
    "df.drop(columns = ['GENDER'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f9e6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There is {df['is_nonsurvivor'].mean()*100:.2f}% of admissions with the patient not surviving.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4328547",
   "metadata": {},
   "source": [
    "# Reducing the number of columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5247d0ab",
   "metadata": {},
   "source": [
    "vasopressor, mechanical_ventilation and rrt_dialysis contain no missing values because they were previously coalesced to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b87abed",
   "metadata": {},
   "source": [
    "## % of nans for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e02d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_df = pd.DataFrame({\n",
    "    'variable': df.columns,\n",
    "    'n_missing': [df[col].isna().sum() for col in df.columns],\n",
    "    'pct_missing': [df[col].isna().mean()*100 for col in df.columns]\n",
    "})\n",
    "\n",
    "# order by porcentage\n",
    "missing_df = missing_df.sort_values(by='pct_missing', ascending=False)\n",
    "\n",
    "print(missing_df.to_string())\n",
    "missing_df.to_csv(\"pct_missing.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386b963d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84ce82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_threshold = 85\n",
    "\n",
    "missing_pct = df.isna().mean() * 100\n",
    "cols_to_drop = missing_pct[missing_pct >= missing_threshold].index\n",
    "print(f\"{len(cols_to_drop)} columns would be dropped due to high missing values (>{missing_threshold}%):\\n\")\n",
    "print(list(cols_to_drop))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82a0065",
   "metadata": {},
   "source": [
    "### Unification of similar or equal variables\n",
    "\n",
    "Some labels from chartevents or labevents or vital signs were codified with different names resulting in different columns for each label name."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74de693f",
   "metadata": {},
   "source": [
    "#### Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348bab5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fahrenheit -> celsius\n",
    "def convert_f_to_c(f_val):\n",
    "    return (f_val - 32) * 5.0/9.0\n",
    "\n",
    "cols_c = [c for c in df.columns if 'Temperature' in c and ('C_' in c or 'Celsius' in c) and 'max' in c]\n",
    "cols_f = [c for c in df.columns if 'Temperature' in c and ('F_' in c or 'Fahrenheit' in c) and 'max' in c]\n",
    "\n",
    "df.drop(columns=cols_c + cols_f, inplace=True)\n",
    "\n",
    "# we do the same for the minimums and maximums\n",
    "for suf in ['mean', 'min']:\n",
    "    df['Vital_Temp_Unified_' + suf] = np.nan\n",
    "    cols_c = [c for c in df.columns if 'Temperature' in c and ('C_' in c or 'Celsius' in c) and suf in c]\n",
    "    cols_f = [c for c in df.columns if 'Temperature' in c and ('F_' in c or 'Fahrenheit' in c) and suf in c]\n",
    "    for c in cols_c:\n",
    "        df['Vital_Temp_Unified_' + suf] = df['Vital_Temp_Unified_' + suf].fillna(df[c])\n",
    "    for c in cols_f:\n",
    "        df['Vital_Temp_Unified_' + suf] = df['Vital_Temp_Unified_' + suf].fillna(df[c].apply(convert_f_to_c))\n",
    "    \n",
    "    df.drop(columns=cols_c + cols_f, inplace=True)\n",
    "\n",
    "missing_pct = df['Vital_Temp_Unified_mean'].isna().mean() * 100\n",
    "print(f\"Vital_Temp_Unified has {missing_pct:.2f}% of missing values\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db38d55a",
   "metadata": {},
   "source": [
    "#### Respiratory Rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cac9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[['Vital_Respiratory Rate (Total)_mean', 'Vital_Resp Rate (Total)_mean', 'Vital_Respiratory Rate_mean']].head(10))\n",
    "\n",
    "for suf in ['_mean', '_min', '_max']:\n",
    "    df['Vital_Respiratory Rate (Total) Unified' + suf] = df['Vital_Respiratory Rate (Total)' + suf].combine_first(\n",
    "        df['Vital_Resp Rate (Total)' + suf]\n",
    "    )  \n",
    "    df.drop(columns=['Vital_Resp Rate (Total)'+ suf, 'Vital_Respiratory Rate (Total)' + suf], inplace=True)\n",
    "    \n",
    "missing_pct = df['Vital_Respiratory Rate (Total) Unified_mean'].isna().mean() * 100\n",
    "print(f\"Vital_Respiratory Rate (Total) Unified has {missing_pct:.2f}% of missing values\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fa1ec2",
   "metadata": {},
   "source": [
    "#### Blood Pressure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da87e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "bp_cols = ['Vital_Manual BP [Systolic]_max', 'Vital_Manual BP [Diastolic]_max', 'Vital_Arterial BP #2 [Systolic]_max',\n",
    "           'Vital_Arterial BP #2 [Diastolic]_max', 'Vital_Arterial BP Mean #2_max','Vital_Arterial Blood Pressure mean_mean',\n",
    "           'Vital_Arterial BP Mean_max', 'Vital_Arterial BP [Systolic]_max', 'Vital_Arterial BP [Diastolic]_max',\n",
    "           'Vital_ART BP mean_max', 'Vital_Arterial Blood Pressure systolic_max', 'Vital_Arterial Blood Pressure diastolic_max']\n",
    "print(df[bp_cols].head())\n",
    "\n",
    "for suf in ['_mean', '_min', '_max']:\n",
    "    df['Vital_Mean Arterial Pressure Unified' + suf] = np.nan\n",
    "\n",
    "    # arterial BP mean first (There are 3 types of codified label)\n",
    "    df['Vital_Mean Arterial Pressure Unified' + suf] = df['Vital_Mean Arterial Pressure Unified' + suf].fillna(df['Vital_Arterial Blood Pressure mean' + suf])\n",
    "    df['Vital_Mean Arterial Pressure Unified' + suf] = df['Vital_Mean Arterial Pressure Unified' + suf].fillna(df['Vital_Arterial BP Mean' + suf])\n",
    "    df['Vital_Mean Arterial Pressure Unified' + suf] = df['Vital_Mean Arterial Pressure Unified' + suf].fillna(df['Vital_ART BP mean' + suf])\n",
    "    \n",
    "    # arterial BP mean #2\n",
    "    df['Vital_Mean Arterial Pressure Unified' + suf] = df['Vital_Mean Arterial Pressure Unified' + suf].fillna(df['Vital_Arterial BP Mean #2' + suf])\n",
    "    \n",
    "    # arterial BP mean computed from arterial BP systolic and diastolic (there are 2 codified labels)\n",
    "    df['Vital_Mean Arterial Pressure Unified' + suf] = df['Vital_Mean Arterial Pressure Unified' + suf].fillna(\n",
    "            (df['Vital_Arterial Blood Pressure systolic' + suf] + 2 * df['Vital_Arterial Blood Pressure diastolic' + suf]) / 3\n",
    "        )\n",
    "    df['Vital_Mean Arterial Pressure Unified' + suf] = df['Vital_Mean Arterial Pressure Unified' + suf].fillna(\n",
    "            (df['Vital_Arterial BP [Systolic]' + suf] + 2 * df['Vital_Arterial BP [Diastolic]' + suf]) / 3\n",
    "        )\n",
    "    \n",
    "    # arterial mean computed from arterial BP systolic and diastolic #2\n",
    "    df['Vital_Mean Arterial Pressure Unified' + suf] = df['Vital_Mean Arterial Pressure Unified' + suf].fillna(\n",
    "            (df['Vital_Arterial BP #2 [Systolic]' + suf] + 2 * df['Vital_Arterial BP #2 [Diastolic]' + suf]) / 3\n",
    "        )\n",
    "    \n",
    "    # arterial mean computed from manual BP (computed)\n",
    "    df['Vital_Mean Arterial Pressure Unified' + suf] = df['Vital_Mean Arterial Pressure Unified' + suf].fillna(df['Vital_Manual BP Mean(calc)' + suf])\n",
    "\n",
    "    # arterial mean computed from manual BP systolic and diastolic \n",
    "    df['Vital_Mean Arterial Pressure Unified' + suf] = df['Vital_Mean Arterial Pressure Unified' + suf].fillna(\n",
    "            (df['Vital_Manual BP [Systolic]' + suf] + 2 * df['Vital_Manual BP [Diastolic]' + suf]) / 3\n",
    "        )\n",
    "\n",
    "bp_cols_to_drop = [\n",
    "    'Vital_Manual BP [Systolic]',\n",
    "    'Vital_Manual BP [Diastolic]',\n",
    "    'Vital_Arterial BP #2 [Systolic]',\n",
    "    'Vital_Arterial BP #2 [Diastolic]',\n",
    "    'Vital_Arterial BP Mean #2',\n",
    "    'Vital_ART BP mean',\n",
    "    'Vital_Arterial BP Mean', \n",
    "    'Vital_Arterial BP [Systolic]', \n",
    "    'Vital_Arterial BP [Diastolic]',\n",
    "    'Vital_Arterial Blood Pressure mean',\n",
    "    'Vital_Arterial Blood Pressure systolic',\n",
    "    'Vital_Arterial Blood Pressure diastolic',\n",
    "    'Vital_Manual BP Mean(calc)'\n",
    "]\n",
    "cols_to_drop = [c + suf for c in bp_cols_to_drop for suf in ['_mean', '_min', '_max'] if c + suf in df.columns]\n",
    "df.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "\n",
    "missing_pct = df['Vital_Mean Arterial Pressure Unified_mean'].isna().mean() * 100\n",
    "print(f\"Vital_Mean Arterial Pressure Unified has {missing_pct:.2f}% of missing values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64c6558",
   "metadata": {},
   "source": [
    "### Sodium, Creatinine, Chloride, Potassium, Blood Urean NItrogen(BUN), Bilirubin,..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2421e046",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_electrolytes = ['Sodium', 'Creatinine', 'Chloride', 'Potassium', 'Magnesium', 'Bilirubin', 'Glucose']\n",
    "\n",
    "for var in vars_electrolytes:\n",
    "    cols = [c for c in df.columns if var in c and 'max' in c]\n",
    "    print(df[cols].head())\n",
    "\n",
    "cols = [c for c in df.columns if ('Urea Nitrogen' in c or 'BUN' in c) and 'max' in c]\n",
    "print(df[cols].head())\n",
    "\n",
    "cols = [c for c in df.columns if ('RBC' in c or 'Red Blood Cells' in c) and 'max' in c]\n",
    "print(df[cols].head())\n",
    "\n",
    "cols = [c for c in df.columns if 'Platelet' in c and 'max' in c]\n",
    "print(df[cols].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6728b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Electrolytes and lab variables to unify\n",
    "vars_electrolytes = ['Sodium', 'Creatinine', 'Chloride', 'Potassium', 'Magnesium', 'Bilirubin', 'Glucose']\n",
    "\n",
    "for var in vars_electrolytes:\n",
    "    for suf in ['max', 'mean', 'min']:\n",
    "        # Find all columns containing the variable and suffix\n",
    "        cols = [c for c in df.columns if var in c and suf in c]\n",
    "        if not cols:\n",
    "            continue\n",
    "\n",
    "        # Prioritize columns starting with 'Lab_'\n",
    "        lab_cols = [c for c in cols if c.startswith('Lab_')]\n",
    "        other_cols = [c for c in cols if not c.startswith('Lab_')]\n",
    "        ordered_cols = lab_cols + other_cols  # Lab first\n",
    "\n",
    "        print(f\"Unifying {var}_{suf}: {cols}\")\n",
    "        \n",
    "        # Create a unified column\n",
    "        unified_col = f'Lab_{var}_Unified_{suf}'\n",
    "        df[unified_col] = np.nan\n",
    "        \n",
    "        # Fill NaNs with values from each column\n",
    "        for col in ordered_cols:\n",
    "            df[unified_col] = df[unified_col].fillna(df[col])\n",
    "\n",
    "        df.drop(columns=cols, inplace=True)\n",
    "\n",
    "# 2. Urea Nitrogen / BUN\n",
    "for suf in ['max', 'mean', 'min']:\n",
    "    cols = [c for c in df.columns if ('Urea Nitrogen' in c or 'BUN' in c) and suf in c]\n",
    "    if not cols:\n",
    "        continue\n",
    "\n",
    "    # Prioritize columns starting with 'Lab_'\n",
    "    lab_cols = [c for c in cols if c.startswith('Lab_')]\n",
    "    other_cols = [c for c in cols if not c.startswith('Lab_')]\n",
    "    ordered_cols = lab_cols + other_cols  # Lab first\n",
    "\n",
    "    print(f\"Unifying Urea/BUN_{suf}: {cols}\")\n",
    "    \n",
    "    unified_col = f'Lab_Urea_Unified_{suf}'\n",
    "    df[unified_col] = np.nan\n",
    "    for col in ordered_cols:\n",
    "        df[unified_col] = df[unified_col].fillna(df[col])\n",
    "\n",
    "    df.drop(columns=cols, inplace=True)\n",
    "\n",
    "# 3. Red Blood Cells (RBC)\n",
    "for suf in ['max', 'mean', 'min']:\n",
    "    cols = [c for c in df.columns if ('RBC' in c or 'Red Blood Cells' in c) and suf in c]\n",
    "    if not cols:\n",
    "        continue\n",
    "\n",
    "    # Prioritize columns starting with 'Lab_'\n",
    "    lab_cols = [c for c in cols if c.startswith('Lab_')]\n",
    "    other_cols = [c for c in cols if not c.startswith('Lab_')]\n",
    "    ordered_cols = lab_cols + other_cols  # Lab first\n",
    "\n",
    "    print(f\"Unifying RBC_{suf}: {cols}\")\n",
    "    \n",
    "    unified_col = f'Lab_Red_Blood_Cells_Unified_{suf}'\n",
    "    df[unified_col] = np.nan\n",
    "    for col in ordered_cols:\n",
    "        df[unified_col] = df[unified_col].fillna(df[col])\n",
    "\n",
    "    df.drop(columns=cols, inplace=True)\n",
    "\n",
    "# 4. Platelets\n",
    "for suf in ['max', 'mean', 'min']:\n",
    "    cols = [c for c in df.columns if 'Platelet' in c and suf in c]\n",
    "    if not cols:\n",
    "        continue\n",
    "    \n",
    "    # Prioritize columns starting with 'Lab_'\n",
    "    lab_cols = [c for c in cols if c.startswith('Lab_')]\n",
    "    other_cols = [c for c in cols if not c.startswith('Lab_')]\n",
    "    ordered_cols = lab_cols + other_cols  # Lab first\n",
    "\n",
    "    print(f\"Unifying Platelets_{suf}: {cols}\")\n",
    "    \n",
    "    unified_col = f'Lab_Platelets_Unified_{suf}'\n",
    "    df[unified_col] = np.nan\n",
    "    for col in ordered_cols:\n",
    "        df[unified_col] = df[unified_col].fillna(df[col])\n",
    "\n",
    "    df.drop(columns=cols, inplace=True)\n",
    "\n",
    "print(\"Original columns dropped, only unified columns remain.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a23a5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee97e768",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_threshold = 80\n",
    "\n",
    "missing_pct = df.isna().mean() * 100\n",
    "cols_to_drop = missing_pct[missing_pct > missing_threshold].index\n",
    "print(f\"{len(cols_to_drop)} columns dropped due to high missing values (>{missing_threshold}%):\\n\")\n",
    "print(list(cols_to_drop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7407dc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary = df.describe().T\n",
    "# summary['cv'] = summary['std'] / summary['mean'] # Coefficient of Variation\n",
    "# summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdce0862",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Vital_Mental status_mean'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b2caf2",
   "metadata": {},
   "source": [
    "## Removing columns of min and max\n",
    "\n",
    "To reduce dimensionality and collinearity, minimum and maximum values were removed for variables with minimal intra-patient variability during the first 24 hours, retaining only the mean value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d5f10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# threshold = 0.99        # relative similarity threshold\n",
    "# min_fraction = 0.10     # minimum fraction of valid rows required\n",
    "# agreement = 0.90       # required agreement percentage\n",
    "\n",
    "# cols = pd.Index(df.columns)\n",
    "\n",
    "# # Identify base variable names (without _mean/_min/_max)\n",
    "# base_vars = set(\n",
    "#     re.sub(r'_(mean|min|max)$', '', c)\n",
    "#     for c in cols\n",
    "#     if re.search(r'_(mean|min|max)$', c)\n",
    "# )\n",
    "\n",
    "# cols_to_drop = []\n",
    "\n",
    "# for var in base_vars:\n",
    "#     mean_col = f\"{var}_mean\"\n",
    "#     min_col  = f\"{var}_min\"\n",
    "#     max_col  = f\"{var}_max\"\n",
    "\n",
    "#     # Skip if not all components exist\n",
    "#     if not all(c in cols for c in [mean_col, min_col, max_col]):\n",
    "#         continue\n",
    "\n",
    "#     mean_vals = df[mean_col]\n",
    "#     min_vals  = df[min_col]\n",
    "#     max_vals  = df[max_col]\n",
    "\n",
    "#     # mean != 0 → use relative ratio\n",
    "#     valid_ratio = (\n",
    "#         mean_vals.notna() &\n",
    "#         min_vals.notna() &\n",
    "#         max_vals.notna() &\n",
    "#         (mean_vals != 0)\n",
    "#     )\n",
    "\n",
    "#     drop_case_A = False\n",
    "#     if valid_ratio.mean() >= min_fraction:\n",
    "#         min_ratio = (min_vals[valid_ratio] / mean_vals[valid_ratio]).abs()\n",
    "#         max_ratio = (max_vals[valid_ratio] / mean_vals[valid_ratio]).abs()\n",
    "\n",
    "#         drop_case_A = (\n",
    "#             (min_ratio > threshold).mean() > agreement and\n",
    "#             (max_ratio > threshold).mean() > agreement\n",
    "#         )\n",
    "\n",
    "#     # if mean == 0 → check flat signals\n",
    "#     valid_zero = (\n",
    "#         mean_vals.eq(0) &\n",
    "#         min_vals.notna() &\n",
    "#         max_vals.notna()\n",
    "#     )\n",
    "\n",
    "#     drop_case_B = False\n",
    "#     if valid_zero.mean() >= min_fraction:\n",
    "#         drop_case_B = (\n",
    "#             (min_vals[valid_zero] == 0).mean() > agreement and\n",
    "#             (max_vals[valid_zero] == 0).mean() > agreement\n",
    "#         )\n",
    "\n",
    "#     if drop_case_A or drop_case_B:\n",
    "#         cols_to_drop.extend([min_col, max_col])\n",
    "\n",
    "# # Drop redundant columns\n",
    "# #df_reduced = df.drop(columns=cols_to_drop)\n",
    "\n",
    "# print(f\"Dropped {len(cols_to_drop)} redundant min/max columns: {cols_to_drop}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe71d61",
   "metadata": {},
   "source": [
    "## Feature correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5192cb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [c for c in df.columns if c not in ('SUBJECT_ID', 'HADM_ID', 'ADMITTIME','DISCHTIME', 'is_nonsurvivor')]\n",
    "print(cols)\n",
    "corr = df[cols].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3fe32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "cols = [c for c in df.columns if c not in ('SUBJECT_ID', 'HADM_ID', 'ADMITTIME','DISCHTIME', 'is_nonsurvivor')]\n",
    "print(cols)\n",
    "\n",
    "survivors = df[df['is_nonsurvivor'] == 0]\n",
    "nonsurvivors = df[df['is_nonsurvivor'] == 1]\n",
    "\n",
    "stats = df.groupby('is_nonsurvivor')[cols].mean().transpose()\n",
    "stats.columns = ['Survivors (Mean)', 'Non-Survivors (Mean)']\n",
    "\n",
    "stats['Total Cohort (Mean)'] = df[cols].mean()\n",
    "stats['% Nans'] = df[cols].isna().mean() * 100\n",
    "\n",
    "# significance of the differences\n",
    "pvals = []\n",
    "for col in cols:\n",
    "    if col != 'is_nonsurvivor': \n",
    "        # remove nans from each group\n",
    "        x = survivors[col].dropna()\n",
    "        y = nonsurvivors[col].dropna()\n",
    "        # test t\n",
    "        if len(x) > 1 and len(y) > 1:\n",
    "            p = ttest_ind(x, y, equal_var=False).pvalue\n",
    "        else:\n",
    "            p = np.nan\n",
    "        pvals.append(p)\n",
    "\n",
    "stats['p-value'] = pvals\n",
    "\n",
    "stats['Significant'] = stats['p-value'] < 0.05  # True if p<0.05\n",
    "stats['p-value'] = stats['p-value'].apply(lambda x: f\"{x:.2e}\" if pd.notna(x) else np.nan)\n",
    "\n",
    "stats = stats[['Total Cohort (Mean)', 'Survivors (Mean)', 'Non-Survivors (Mean)',\n",
    "                 '% Nans', 'p-value', 'Significant']]\n",
    "\n",
    "display(stats)\n",
    "stats.to_csv('statistics_survivors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165bb46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_cols = stats[(stats['Significant']) & (stats['% Nans'] < 60)].index.tolist()\n",
    "print(len(significant_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e809ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corr_matrix = df[significant_cols].corr().abs()  # matriz de correlación absoluta\n",
    "# to_drop = set()\n",
    "# threshold = 0.99\n",
    "\n",
    "# for i in range(len(corr_matrix.columns)):\n",
    "#     for j in range(i):\n",
    "#         if corr_matrix.iloc[i, j] > threshold:\n",
    "#             colname = corr_matrix.columns[i]\n",
    "#             to_drop.add(colname)\n",
    "\n",
    "# print(f\"there are {len(to_drop)} : {to_drop}\")\n",
    "# final_cols = [c for c in significant_cols if c not in to_drop]\n",
    "# print(f\"{len(final_cols)} signficant columns \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70385fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats[(stats['% Nans'] < 60) & (stats.index.str.endswith('_mean'))]['Significant'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866d14a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_mean_cols = stats[(stats['Significant']) & (stats['% Nans'] < 60) & (stats.index.str.endswith('_mean'))].index.tolist()\n",
    "print(len(significant_mean_cols))\n",
    "\n",
    "corr_matrix = df[significant_mean_cols].corr().abs() \n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr_matrix,\n",
    "            annot=False,\n",
    "            fmt=\".1f\", # Round to 2 decimal places\n",
    "            cmap='coolwarm',\n",
    "            center=0,\n",
    "            vmin=-1,\n",
    "            vmax=1,\n",
    "            linewidths=0.5)\n",
    "plt.title('Feature correlation heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4bf836",
   "metadata": {},
   "source": [
    "## Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7275b381",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = df.quantile(0.25)\n",
    "q3 = df.quantile(0.75)\n",
    "iqr = q3 - q1\n",
    "outlier_counts = ((df < (q1 - 1.5 * iqr)) | (df > (q3 + 1.5 * iqr))).sum()\n",
    "print(\"\\nNumber of Outliers per Feature:\\n\", outlier_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06ddde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Variables significativas\n",
    "significant_mean_cols = stats[(stats['Significant']) & (stats['% Nans'] < 60) & (stats.index.str.endswith('_mean'))].index.tolist()\n",
    "\n",
    "# Dividir en varias figuras si son muchas\n",
    "plots_per_fig = 12\n",
    "for i in range(0, len(significant_mean_cols), plots_per_fig):\n",
    "    cols_subset = significant_mean_cols[i:i+plots_per_fig]\n",
    "    n_cols = 4\n",
    "    n_rows = int(np.ceil(len(cols_subset) / n_cols))\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(4*n_cols, 3*n_rows))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for ax, col in zip(axes, cols_subset):\n",
    "        # Use seaborn.histplot\n",
    "        sns.histplot(\n",
    "            data=df,\n",
    "            x=col,\n",
    "            hue='is_nonsurvivor',\n",
    "            kde=True,\n",
    "            palette={0:'lightblue', 1:'plum'}, # survivor=0, non-survivor=1\n",
    "            alpha=0.6,\n",
    "            ax=ax,\n",
    "            element='step',  # step or bars\n",
    "            common_norm=False # separate normalization for each hue\n",
    "        )\n",
    "        ax.set_title(col, fontsize=10)\n",
    "\n",
    "        # Calculate % of NaNs per group\n",
    "        pct_nan_surv = df[df['is_nonsurvivor']==0][col].isna().mean() * 100\n",
    "        pct_nan_nonsurv = df[df['is_nonsurvivor']==1][col].isna().mean() * 100\n",
    "        \n",
    "        # Annotate on the plot\n",
    "        ax.text(\n",
    "            0.35, 0.95,  # x=0.65 para dejar espacio para la leyenda\n",
    "            f\"NaN %\\nSurv: {pct_nan_surv:.1f}%\\nNon-Surv: {pct_nan_nonsurv:.1f}%\",\n",
    "            horizontalalignment='left', \n",
    "            verticalalignment='top',\n",
    "            transform=ax.transAxes, \n",
    "            fontsize=6, \n",
    "            bbox=dict(facecolor='white', alpha=0.5)\n",
    "        )\n",
    "    \n",
    "    # Remove empty axes\n",
    "    for ax in axes[len(cols_subset):]:\n",
    "        ax.remove()\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e333d44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Significant features\n",
    "significant_cols = stats[(stats['Significant']) & (stats['% Nans'] < 60)].index.tolist()\n",
    "print(len(significant_cols))\n",
    "\n",
    "corr_with_target = df[significant_cols + ['is_nonsurvivor']].corr()['is_nonsurvivor'].drop('is_nonsurvivor').sort_values(ascending=False)\n",
    "print(corr_with_target.shape)\n",
    "\n",
    "plt.figure(figsize=(6, 10))\n",
    "sns.heatmap(\n",
    "    corr_with_target.to_frame(),\n",
    "    cmap=sns.diverging_palette(250, 345, as_cmap=True),\n",
    "    center=0,\n",
    "    annot=False,\n",
    "    fmt=\".3f\"\n",
    ")\n",
    "plt.title('Correlation with diagnosis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537ffebe",
   "metadata": {},
   "source": [
    "### Graphics for the binary features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b55cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Graphics for the binary features\n",
    "binary_cols = ['vasopressor', 'mechanical_ventilation', 'rrt_dialysis']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ab4a4a",
   "metadata": {},
   "source": [
    "# Imputation of nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c393203",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_90 = df[df.isna().mean(axis=1) < 0.1]  # <10% missing\n",
    "df_90.shape \n",
    "\n",
    "# w ecan not use the combine method for imputing nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fdb4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define groups\n",
    "groups = {'survivor': 0, 'non_survivor': 1}\n",
    "\n",
    "# Threshold for deciding median vs mean\n",
    "outlier_threshold = 50\n",
    "\n",
    "# Loop over all columns you want to impute\n",
    "for col in df.columns:\n",
    "    \n",
    "    # Skip target and IDs\n",
    "    if col in ['SUBJECT_ID', 'HADM_ID', 'DISCHTIME', 'ADMITTIME', 'is_nonsurvivor', 'length_of_stay_days', 'vasopressor', 'mechanical_ventilation',\t'rrt_dialysis']:\n",
    "        continue\n",
    "    \n",
    "    # Calculate number of extreme outliers (e.g., outside 1.5*IQR)\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    outliers_count = ((df[col] < Q1 - 1.5*IQR) | (df[col] > Q3 + 1.5*IQR)).sum()\n",
    "    \n",
    "    # Decide imputation function\n",
    "    if outliers_count > outlier_threshold:\n",
    "        impute_func = np.median\n",
    "    else:\n",
    "        impute_func = np.mean\n",
    "    \n",
    "    # Impute per group\n",
    "    for label, val in groups.items():\n",
    "        mask = df['is_nonsurvivor'] == val\n",
    "        impute_value = impute_func(df.loc[mask, col])\n",
    "        df.loc[mask & df[col].isna(), col] = impute_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ab1f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in significant_cols:\n",
    "    sns.boxplot(\n",
    "        data=df,\n",
    "        x='is_nonsurvivor',\n",
    "        y=col\n",
    "    )\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Anaconda base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
